import sys,csvfrom pyspark import SparkContextimport itertoolsfrom  collections import defaultdictimport mathcandidate_set=frozenset()frequent_item_sets=set()from operator import addsc = SparkContext(appName="SON_Apriori")input_file = sys.argv[1]support_threshold = float(sys.argv[2])global_frequent_set = []def Items_with_minimum_threshold(item_set, baskets):        for_support=support_threshold * len(baskets)        support_threshold_1 = math.ceil(for_support)        frequent_item_set = set()        tot_items = defaultdict(int)        for basket in baskets:            for item in item_set:                if item.issubset(basket):                    tot_items[item]=tot_items[item] + 1        for key, count in tot_items.items():            if count >= support_threshold_1:                frequent_item_set.add(key)        return frequent_item_setdef construct_candidate_sets(frequent_set, kth):        new_candidate_set = set()        for item_k in frequent_set:            for item_j in frequent_set:                new_candidate = item_k.union(item_j)                if len(new_candidate) == kth:                    new_candidate_set.add(new_candidate)        return new_candidate_setdef Apriori(file):    item_set=set()    baskets=[]    for transaction in file:        items = transaction.split(',')        baskets.append(frozenset(items))        length_of_baskets = len(baskets)        for item in items:            item_set.add(frozenset([item]))    one_frequent_item_set =Items_with_minimum_threshold(item_set, baskets)    frequent_item_sets.update(one_frequent_item_set)    current_frequent_set = one_frequent_item_set    k = 2    length_frq_sets=len(current_frequent_set)    while length_frq_sets > 0:        candidate_set= construct_candidate_sets(current_frequent_set, k)        current_frequent_set = Items_with_minimum_threshold(candidate_set, baskets)        frequent_item_sets.update(current_frequent_set)        k += 1    return frequent_item_setsdef find_global_frequent_sets(each_list):        candidate_set_count = defaultdict(int)        for transaction in each_list:            basket = frozenset(transaction.split(','))            for candidate in candidate_set:                if candidate.issubset(basket):                    candidate_set_count[candidate] +=1        yield candidate_set_countrdd = sc.textFile(input_file)candidate_set = frozenset(rdd.mapPartitions(Apriori).collect())#candidate_set = frozenset(candidate_set)candid = sc.parallelize(rdd.mapPartitions(find_global_frequent_sets).collect()).flatMap(lambda x: x.items()).reduceByKey(lambda x,y: x+y).collect()#print ccount_1 =rdd.count()support_threshold_act= support_threshold * count_1for key,value in candid:    if value >= support_threshold_act:        global_frequent_set.append(sorted(list(key)))        #global_frequent_set.sort()#print global_frequent_setoutput_file = open(sys.argv[3],"w")#file_writer = csv.writer(output_file)for frequent_items in sorted(global_frequent_set):    print frequent_items    frequent_items.sort()    frequent_items=str(frequent_items)    #print frequent_items    output_file.write(frequent_items+"\n")