import sysfrom pyspark import SparkContextimport itertoolsfrom  collections import defaultdictimport mathcandidate_set=frozenset()frequent_item_sets=set()from operator import addsc = SparkContext(appName="SON_Apriori")input_file = sys.argv[1]support_threshold = float(sys.argv[2])def Items_with_minimum_threshold(item_set, baskets):        support_threshold_1 = math.ceil(support_threshold * len(baskets))        frequent_item_set = set()        tot_items = defaultdict(int)        for basket in baskets:            for item in item_set:                if item.issubset(basket):                    tot_items[item]=tot_items[item] + 1        for item, count in tot_items.items():            if count >= support_threshold_1:                frequent_item_set.add(item)        return frequent_item_setdef construct_candidate_sets(frequent_set, kth):        new_candidate_set = set()        for item_k in frequent_set:            for item_j in frequent_set:                new_candidate = item_k.union(item_j)                if len(new_candidate) == kth:                    new_candidate_set.add(new_candidate)        return new_candidate_setdef find_global_frequent_sets(transaction_list):        candidate_set_count = defaultdict(int)        for transaction in transaction_list:            basket = frozenset(transaction.split(','))            for candidate in candidate_set:                if candidate.issubset(basket):                    candidate_set_count[candidate] += 1        yield candidate_set_countdef Apriori(file):    item_set=set()    baskets=[]    for transaction in file:        items = transaction.split(',')        baskets.append(frozenset(items))        length_of_baskets = len(baskets)        for item in items:            item_set.add(frozenset([item]))    one_frequent_item_set =Items_with_minimum_threshold(item_set, baskets)    frequent_item_sets.update(one_frequent_item_set)    current_frequent_set = one_frequent_item_set    ith_item_set = 2    #length_frq_sets=len(current_frequent_set)    while len(current_frequent_set) > 0:        candidate_set= construct_candidate_sets(current_frequent_set, ith_item_set)        current_frequent_set = Items_with_minimum_threshold(candidate_set, baskets)        frequent_item_sets.update(current_frequent_set)        ith_item_set += 1    return frequent_item_setsglobal_frequent_set = []rdd = sc.textFile(input_file)candidate_set = frozenset(rdd.mapPartitions(Apriori).collect())c = sc.parallelize(rdd.mapPartitions(find_global_frequent_sets).collect()).flatMap(lambda x: x.items()).reduceByKey(lambda x,y: x+y).collect()#count_1 =rdd.count()s_T= support_threshold * rdd.count()for item,value in c:    if value >= s_T:        global_frequent_set.append(sorted(list(item)))print global_frequent_setoutput_file = open(sys.argv[3],"w")#file_writer = csv.writer(output_file)for frequent_items in sorted(global_frequent_set):    print frequent_items    frequent_items.sort()    frequent_items=str(frequent_items)    output_file.write(frequent_items+"\n")